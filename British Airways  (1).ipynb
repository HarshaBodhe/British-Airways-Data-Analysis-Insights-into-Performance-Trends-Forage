{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9331b0c8-fe51-4f0b-bdb9-cf657ccc2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d8e471c3-e948-4c76-9810-c8b12e0abc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1 with 10 reviews.\n",
      "Scraped page 2 with 10 reviews.\n",
      "Scraped page 3 with 10 reviews.\n",
      "Scraped page 4 with 10 reviews.\n",
      "Scraped page 5 with 10 reviews.\n",
      "Scraped page 6 with 10 reviews.\n",
      "Scraped page 7 with 10 reviews.\n",
      "Scraped page 8 with 10 reviews.\n",
      "Scraped page 9 with 10 reviews.\n",
      "Scraped page 10 with 10 reviews.\n",
      "Scraped page 11 with 10 reviews.\n",
      "Scraped page 12 with 10 reviews.\n",
      "Scraped page 13 with 10 reviews.\n",
      "Scraped page 14 with 10 reviews.\n",
      "Scraped page 15 with 10 reviews.\n",
      "Scraped page 16 with 10 reviews.\n",
      "Scraped page 17 with 10 reviews.\n",
      "Scraped page 18 with 10 reviews.\n",
      "Scraped page 19 with 10 reviews.\n",
      "Scraped page 20 with 10 reviews.\n",
      "Scraped page 21 with 10 reviews.\n",
      "Scraped page 22 with 10 reviews.\n",
      "Scraped page 23 with 10 reviews.\n",
      "Scraped page 24 with 10 reviews.\n",
      "Scraped page 25 with 10 reviews.\n",
      "Scraped page 26 with 10 reviews.\n",
      "Scraped page 27 with 10 reviews.\n",
      "Scraped page 28 with 10 reviews.\n",
      "Scraped page 29 with 10 reviews.\n",
      "Scraped page 30 with 10 reviews.\n",
      "Scraped page 31 with 10 reviews.\n",
      "Scraped page 32 with 10 reviews.\n",
      "Scraped page 33 with 10 reviews.\n",
      "Scraped page 34 with 10 reviews.\n",
      "Scraped page 35 with 10 reviews.\n",
      "Scraped page 36 with 10 reviews.\n",
      "Scraped page 37 with 10 reviews.\n",
      "Scraped page 38 with 10 reviews.\n",
      "Scraped page 39 with 10 reviews.\n",
      "Scraped page 40 with 10 reviews.\n",
      "Scraped page 41 with 10 reviews.\n",
      "Scraped page 42 with 10 reviews.\n",
      "Scraped page 43 with 10 reviews.\n",
      "Scraped page 44 with 10 reviews.\n",
      "Scraped page 45 with 10 reviews.\n",
      "Scraped page 46 with 10 reviews.\n",
      "Scraped page 47 with 10 reviews.\n",
      "Scraped page 48 with 10 reviews.\n",
      "Scraped page 49 with 10 reviews.\n",
      "Scraped page 50 with 10 reviews.\n",
      "Scraped page 51 with 10 reviews.\n",
      "Scraped page 52 with 10 reviews.\n",
      "Scraped page 53 with 10 reviews.\n",
      "Scraped page 54 with 10 reviews.\n",
      "Scraped page 55 with 10 reviews.\n",
      "Scraped page 56 with 10 reviews.\n",
      "Scraped page 57 with 10 reviews.\n",
      "Scraped page 58 with 10 reviews.\n",
      "Scraped page 59 with 10 reviews.\n",
      "Scraped page 60 with 10 reviews.\n",
      "Scraped page 61 with 10 reviews.\n",
      "Scraped page 62 with 10 reviews.\n",
      "Scraped page 63 with 10 reviews.\n",
      "Scraped page 64 with 10 reviews.\n",
      "Scraped page 65 with 10 reviews.\n",
      "Scraped page 66 with 10 reviews.\n",
      "Scraped page 67 with 10 reviews.\n",
      "Scraped page 68 with 10 reviews.\n",
      "Scraped page 69 with 10 reviews.\n",
      "Scraped page 70 with 10 reviews.\n",
      "Scraped page 71 with 10 reviews.\n",
      "Scraped page 72 with 10 reviews.\n",
      "Scraped page 73 with 10 reviews.\n",
      "Scraped page 74 with 10 reviews.\n",
      "Scraped page 75 with 10 reviews.\n",
      "Scraped page 76 with 10 reviews.\n",
      "Scraped page 77 with 10 reviews.\n",
      "Scraped page 78 with 10 reviews.\n",
      "Scraped page 79 with 10 reviews.\n",
      "Scraped page 80 with 10 reviews.\n",
      "Scraped page 81 with 10 reviews.\n",
      "Scraped page 82 with 10 reviews.\n",
      "Scraped page 83 with 10 reviews.\n",
      "Scraped page 84 with 10 reviews.\n",
      "Scraped page 85 with 10 reviews.\n",
      "Scraped page 86 with 10 reviews.\n",
      "Scraped page 87 with 10 reviews.\n",
      "Scraped page 88 with 10 reviews.\n",
      "Scraped page 89 with 10 reviews.\n",
      "Scraped page 90 with 10 reviews.\n",
      "Scraped page 91 with 10 reviews.\n",
      "Scraped page 92 with 10 reviews.\n",
      "Scraped page 93 with 10 reviews.\n",
      "Scraped page 94 with 10 reviews.\n",
      "Scraped page 95 with 10 reviews.\n",
      "Scraped page 96 with 10 reviews.\n",
      "Scraped page 97 with 10 reviews.\n",
      "Scraped page 98 with 10 reviews.\n",
      "Scraped page 99 with 10 reviews.\n",
      "Scraped page 100 with 10 reviews.\n",
      "Scraped page 101 with 10 reviews.\n",
      "Scraped page 102 with 10 reviews.\n",
      "Scraped page 103 with 10 reviews.\n",
      "Scraped page 104 with 10 reviews.\n",
      "Scraped page 105 with 10 reviews.\n",
      "Scraped page 106 with 10 reviews.\n",
      "Scraped page 107 with 10 reviews.\n",
      "Scraped page 108 with 10 reviews.\n",
      "Scraped page 109 with 10 reviews.\n",
      "Scraped page 110 with 10 reviews.\n",
      "Scraped page 111 with 10 reviews.\n",
      "Scraped page 112 with 10 reviews.\n",
      "Scraped page 113 with 10 reviews.\n",
      "Scraped page 114 with 10 reviews.\n",
      "Scraped page 115 with 10 reviews.\n",
      "Scraped page 116 with 10 reviews.\n",
      "Scraped page 117 with 10 reviews.\n",
      "Scraped page 118 with 10 reviews.\n",
      "Scraped page 119 with 10 reviews.\n",
      "Scraped page 120 with 10 reviews.\n",
      "Scraped page 121 with 10 reviews.\n",
      "Scraped page 122 with 10 reviews.\n",
      "Scraped page 123 with 10 reviews.\n",
      "Scraped page 124 with 10 reviews.\n",
      "Scraped page 125 with 10 reviews.\n",
      "Scraped page 126 with 10 reviews.\n",
      "Scraped page 127 with 10 reviews.\n",
      "Scraped page 128 with 10 reviews.\n",
      "Scraped page 129 with 10 reviews.\n",
      "Scraped page 130 with 10 reviews.\n",
      "Scraped page 131 with 10 reviews.\n",
      "Scraped page 132 with 10 reviews.\n",
      "Scraped page 133 with 10 reviews.\n",
      "Scraped page 134 with 10 reviews.\n",
      "Scraped page 135 with 10 reviews.\n",
      "Scraped page 136 with 10 reviews.\n",
      "Scraped page 137 with 10 reviews.\n",
      "Scraped page 138 with 10 reviews.\n",
      "Scraped page 139 with 10 reviews.\n",
      "Scraped page 140 with 10 reviews.\n",
      "Scraped page 141 with 10 reviews.\n",
      "Scraped page 142 with 10 reviews.\n",
      "Scraped page 143 with 10 reviews.\n",
      "Scraped page 144 with 10 reviews.\n",
      "Scraped page 145 with 10 reviews.\n",
      "Scraped page 146 with 10 reviews.\n",
      "Scraped page 147 with 10 reviews.\n",
      "Scraped page 148 with 10 reviews.\n",
      "Scraped page 149 with 10 reviews.\n",
      "Scraped page 150 with 10 reviews.\n",
      "Scraped page 151 with 10 reviews.\n",
      "Scraped page 152 with 10 reviews.\n",
      "Scraped page 153 with 10 reviews.\n",
      "Scraped page 154 with 10 reviews.\n",
      "Scraped page 155 with 10 reviews.\n",
      "Scraped page 156 with 10 reviews.\n",
      "Scraped page 157 with 10 reviews.\n",
      "Scraped page 158 with 10 reviews.\n",
      "Scraped page 159 with 10 reviews.\n",
      "Scraped page 160 with 10 reviews.\n",
      "Scraped page 161 with 10 reviews.\n",
      "Scraped page 162 with 10 reviews.\n",
      "Scraped page 163 with 10 reviews.\n",
      "Scraped page 164 with 10 reviews.\n",
      "Scraped page 165 with 10 reviews.\n",
      "Scraped page 166 with 10 reviews.\n",
      "Scraped page 167 with 10 reviews.\n",
      "Scraped page 168 with 10 reviews.\n",
      "Scraped page 169 with 10 reviews.\n",
      "Scraped page 170 with 10 reviews.\n",
      "Scraped page 171 with 10 reviews.\n",
      "Scraped page 172 with 10 reviews.\n",
      "Scraped page 173 with 10 reviews.\n",
      "Scraped page 174 with 10 reviews.\n",
      "Scraped page 175 with 10 reviews.\n",
      "Scraped page 176 with 10 reviews.\n",
      "Scraped page 177 with 10 reviews.\n",
      "Scraped page 178 with 10 reviews.\n",
      "Scraped page 179 with 10 reviews.\n",
      "Scraped page 180 with 10 reviews.\n",
      "Scraped page 181 with 10 reviews.\n",
      "Scraped page 182 with 10 reviews.\n",
      "Scraped page 183 with 10 reviews.\n",
      "Scraped page 184 with 10 reviews.\n",
      "Scraped page 185 with 10 reviews.\n",
      "Scraped page 186 with 10 reviews.\n",
      "Scraped page 187 with 10 reviews.\n",
      "Scraped page 188 with 10 reviews.\n",
      "Scraped page 189 with 10 reviews.\n",
      "Scraped page 190 with 10 reviews.\n",
      "Scraped page 191 with 10 reviews.\n",
      "Scraped page 192 with 10 reviews.\n",
      "Scraped page 193 with 10 reviews.\n",
      "Scraped page 194 with 10 reviews.\n",
      "Scraped page 195 with 10 reviews.\n",
      "Scraped page 196 with 10 reviews.\n",
      "Scraped page 197 with 10 reviews.\n",
      "Scraped page 198 with 10 reviews.\n",
      "Scraped page 199 with 10 reviews.\n",
      "Scraped page 200 with 10 reviews.\n",
      "Scraped page 201 with 10 reviews.\n",
      "Scraped page 202 with 10 reviews.\n",
      "Scraped page 203 with 10 reviews.\n",
      "Scraped page 204 with 10 reviews.\n",
      "Scraped page 205 with 10 reviews.\n",
      "Scraped page 206 with 10 reviews.\n",
      "Scraped page 207 with 10 reviews.\n",
      "Scraped page 208 with 10 reviews.\n",
      "Scraped page 209 with 10 reviews.\n",
      "Scraped page 210 with 10 reviews.\n",
      "Scraped page 211 with 10 reviews.\n",
      "Scraped page 212 with 10 reviews.\n",
      "Scraped page 213 with 10 reviews.\n",
      "Scraped page 214 with 10 reviews.\n",
      "Scraped page 215 with 10 reviews.\n",
      "Scraped page 216 with 10 reviews.\n",
      "Scraped page 217 with 10 reviews.\n",
      "Scraped page 218 with 10 reviews.\n",
      "Scraped page 219 with 10 reviews.\n",
      "Scraped page 220 with 10 reviews.\n",
      "Scraped page 221 with 10 reviews.\n",
      "Scraped page 222 with 10 reviews.\n",
      "Scraped page 223 with 10 reviews.\n",
      "Scraped page 224 with 10 reviews.\n",
      "Scraped page 225 with 10 reviews.\n",
      "Scraped page 226 with 10 reviews.\n",
      "Scraped page 227 with 10 reviews.\n",
      "Scraped page 228 with 10 reviews.\n",
      "Scraped page 229 with 10 reviews.\n",
      "Scraped page 230 with 10 reviews.\n",
      "Scraped page 231 with 10 reviews.\n",
      "Scraped page 232 with 10 reviews.\n",
      "Scraped page 233 with 10 reviews.\n",
      "Scraped page 234 with 10 reviews.\n",
      "Scraped page 235 with 10 reviews.\n",
      "Scraped page 236 with 10 reviews.\n",
      "Scraped page 237 with 10 reviews.\n",
      "Scraped page 238 with 10 reviews.\n",
      "Scraped page 239 with 10 reviews.\n",
      "Scraped page 240 with 10 reviews.\n",
      "Scraped page 241 with 10 reviews.\n",
      "Scraped page 242 with 10 reviews.\n",
      "Scraped page 243 with 10 reviews.\n",
      "Scraped page 244 with 10 reviews.\n",
      "Scraped page 245 with 10 reviews.\n",
      "Scraped page 246 with 10 reviews.\n",
      "Scraped page 247 with 10 reviews.\n",
      "Scraped page 248 with 10 reviews.\n",
      "Scraped page 249 with 10 reviews.\n",
      "Scraped page 250 with 10 reviews.\n",
      "Scraped page 251 with 10 reviews.\n",
      "Scraped page 252 with 10 reviews.\n",
      "Scraped page 253 with 10 reviews.\n",
      "Scraped page 254 with 10 reviews.\n",
      "Scraped page 255 with 10 reviews.\n",
      "Scraped page 256 with 10 reviews.\n",
      "Scraped page 257 with 10 reviews.\n",
      "Scraped page 258 with 10 reviews.\n",
      "Scraped page 259 with 10 reviews.\n",
      "Scraped page 260 with 10 reviews.\n",
      "Scraped page 261 with 10 reviews.\n",
      "Scraped page 262 with 10 reviews.\n",
      "Scraped page 263 with 10 reviews.\n",
      "Scraped page 264 with 10 reviews.\n",
      "Scraped page 265 with 10 reviews.\n",
      "Scraped page 266 with 10 reviews.\n",
      "Scraped page 267 with 10 reviews.\n",
      "Scraped page 268 with 10 reviews.\n",
      "Scraped page 269 with 10 reviews.\n",
      "Scraped page 270 with 10 reviews.\n",
      "Scraped page 271 with 10 reviews.\n",
      "Scraped page 272 with 10 reviews.\n",
      "Scraped page 273 with 10 reviews.\n",
      "Scraped page 274 with 10 reviews.\n",
      "Scraped page 275 with 10 reviews.\n",
      "Scraped page 276 with 10 reviews.\n",
      "Scraped page 277 with 10 reviews.\n",
      "Scraped page 278 with 10 reviews.\n",
      "Scraped page 279 with 10 reviews.\n",
      "Scraped page 280 with 10 reviews.\n",
      "Scraped page 281 with 10 reviews.\n",
      "Scraped page 282 with 10 reviews.\n",
      "Scraped page 283 with 10 reviews.\n",
      "Scraped page 284 with 10 reviews.\n",
      "Scraped page 285 with 10 reviews.\n",
      "Scraped page 286 with 10 reviews.\n",
      "Scraped page 287 with 10 reviews.\n",
      "Scraped page 288 with 10 reviews.\n",
      "Scraped page 289 with 10 reviews.\n",
      "Scraped page 290 with 10 reviews.\n",
      "Scraped page 291 with 10 reviews.\n",
      "Scraped page 292 with 10 reviews.\n",
      "Scraped page 293 with 10 reviews.\n",
      "Scraped page 294 with 10 reviews.\n",
      "Scraped page 295 with 10 reviews.\n",
      "Scraped page 296 with 10 reviews.\n",
      "Scraped page 297 with 10 reviews.\n",
      "Scraped page 298 with 10 reviews.\n",
      "Scraped page 299 with 10 reviews.\n",
      "Scraped page 300 with 10 reviews.\n",
      "Scraped page 301 with 10 reviews.\n",
      "Scraped page 302 with 10 reviews.\n",
      "Scraped page 303 with 10 reviews.\n",
      "Scraped page 304 with 10 reviews.\n",
      "Scraped page 305 with 10 reviews.\n",
      "Scraped page 306 with 10 reviews.\n",
      "Scraped page 307 with 10 reviews.\n",
      "Scraped page 308 with 10 reviews.\n",
      "Scraped page 309 with 10 reviews.\n",
      "Scraped page 310 with 10 reviews.\n",
      "Scraped page 311 with 10 reviews.\n",
      "Scraped page 312 with 10 reviews.\n",
      "Scraped page 313 with 10 reviews.\n",
      "Scraped page 314 with 10 reviews.\n",
      "Scraped page 315 with 10 reviews.\n",
      "Scraped page 316 with 10 reviews.\n",
      "Scraped page 317 with 10 reviews.\n",
      "Scraped page 318 with 10 reviews.\n",
      "Scraped page 319 with 10 reviews.\n",
      "Scraped page 320 with 10 reviews.\n",
      "Scraped page 321 with 10 reviews.\n",
      "Scraped page 322 with 10 reviews.\n",
      "Scraped page 323 with 10 reviews.\n",
      "Scraped page 324 with 10 reviews.\n",
      "Scraped page 325 with 10 reviews.\n",
      "Scraped page 326 with 10 reviews.\n",
      "Scraped page 327 with 10 reviews.\n",
      "Scraped page 328 with 10 reviews.\n",
      "Scraped page 329 with 10 reviews.\n",
      "Scraped page 330 with 10 reviews.\n",
      "Scraped page 331 with 10 reviews.\n",
      "Scraped page 332 with 10 reviews.\n",
      "Scraped page 333 with 10 reviews.\n",
      "Scraped page 334 with 10 reviews.\n",
      "Scraped page 335 with 10 reviews.\n",
      "Scraped page 336 with 10 reviews.\n",
      "Scraped page 337 with 10 reviews.\n",
      "Scraped page 338 with 10 reviews.\n",
      "Scraped page 339 with 10 reviews.\n",
      "Scraped page 340 with 10 reviews.\n",
      "Scraped page 341 with 10 reviews.\n",
      "Scraped page 342 with 10 reviews.\n",
      "Scraped page 343 with 10 reviews.\n",
      "Scraped page 344 with 10 reviews.\n",
      "Scraped page 345 with 10 reviews.\n",
      "Scraped page 346 with 10 reviews.\n",
      "Scraped page 347 with 10 reviews.\n",
      "Scraped page 348 with 10 reviews.\n",
      "Scraped page 349 with 10 reviews.\n",
      "Scraped page 350 with 10 reviews.\n",
      "Scraped page 351 with 10 reviews.\n",
      "Scraped page 352 with 10 reviews.\n",
      "Scraped page 353 with 10 reviews.\n",
      "Scraped page 354 with 10 reviews.\n",
      "Scraped page 355 with 10 reviews.\n",
      "Scraped page 356 with 10 reviews.\n",
      "Scraped page 357 with 10 reviews.\n",
      "Scraped page 358 with 10 reviews.\n",
      "Scraped page 359 with 10 reviews.\n",
      "Scraped page 360 with 10 reviews.\n",
      "Scraped page 361 with 10 reviews.\n",
      "Scraped page 362 with 10 reviews.\n",
      "Scraped page 363 with 10 reviews.\n",
      "Scraped page 364 with 10 reviews.\n",
      "Scraped page 365 with 10 reviews.\n",
      "Scraped page 366 with 10 reviews.\n",
      "Scraped page 367 with 10 reviews.\n",
      "Scraped page 368 with 10 reviews.\n",
      "Scraped page 369 with 10 reviews.\n",
      "Scraped page 370 with 10 reviews.\n",
      "Scraped page 371 with 10 reviews.\n",
      "Scraped page 372 with 10 reviews.\n",
      "Scraped page 373 with 10 reviews.\n",
      "Scraped page 374 with 10 reviews.\n",
      "Scraped page 375 with 10 reviews.\n",
      "Scraped page 376 with 10 reviews.\n",
      "Scraped page 377 with 10 reviews.\n",
      "Scraped page 378 with 10 reviews.\n",
      "Scraped page 379 with 10 reviews.\n",
      "Scraped page 380 with 10 reviews.\n",
      "Scraped page 381 with 10 reviews.\n",
      "Scraped page 382 with 10 reviews.\n",
      "Scraped page 383 with 10 reviews.\n",
      "Scraped page 384 with 10 reviews.\n",
      "Scraped page 385 with 10 reviews.\n",
      "Scraped page 386 with 10 reviews.\n",
      "Scraped page 387 with 10 reviews.\n",
      "Scraped page 388 with 10 reviews.\n",
      "Scraped page 389 with 10 reviews.\n",
      "Scraped page 390 with 10 reviews.\n",
      "Scraped page 391 with 10 reviews.\n",
      "Scraped page 392 with 9 reviews.\n",
      "No more reviews found, stopping.\n",
      "All reviews saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways/page/\"\n",
    "page = 1  # Start from page 1\n",
    "all_reviews = []\n",
    "\n",
    "while True:\n",
    "    url = f\"{base_url}{page}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}, stopping.\")\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    reviews = soup.find_all('div', class_='text_content')\n",
    "    \n",
    "    if not reviews:\n",
    "        print(\"No more reviews found, stopping.\")\n",
    "        break\n",
    "    \n",
    "    # Extract review text\n",
    "    review_texts = [review.get_text(strip=True) for review in reviews]\n",
    "    all_reviews.extend(review_texts)\n",
    "    \n",
    "    print(f\"Scraped page {page} with {len(review_texts)} reviews.\")\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Save to CSV file if we have reviews\n",
    "if all_reviews:\n",
    "    df = pd.DataFrame({'Review': all_reviews})\n",
    "    df.to_csv(\"BA_reviews.csv\", index=False)\n",
    "    print(\"All reviews saved successfully!\")\n",
    "else:\n",
    "    print(\"No reviews collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5bdbb975-997b-4416-8ca9-3a34005d9cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[✅ , [[&lt;em&gt;Trip Verified&lt;/em&gt;]],  |   The seat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[✅ , [[&lt;em&gt;Trip Verified&lt;/em&gt;]],  |   After th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[✅ , [[&lt;em&gt;Trip Verified&lt;/em&gt;]],  |   Prior to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[✅ , [[&lt;em&gt;Trip Verified&lt;/em&gt;]],  |   I flew f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[✅ , [[&lt;em&gt;Trip Verified&lt;/em&gt;]],  | First the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  [✅ , [[<em>Trip Verified</em>]],  |   The seat...\n",
       "1  [✅ , [[<em>Trip Verified</em>]],  |   After th...\n",
       "2  [✅ , [[<em>Trip Verified</em>]],  |   Prior to...\n",
       "3  [✅ , [[<em>Trip Verified</em>]],  |   I flew f...\n",
       "4  [✅ , [[<em>Trip Verified</em>]],  | First the ..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb0fa10d-d67f-4934-937a-9d72f24ab625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Create 'data' directory if it does not exist\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# Now save the file\n",
    "df.to_csv(\"data/BA_reviews.csv\", index=False)\n",
    "print(\"File saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28acb926-cb9c-472f-95b5-fdc4423377b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "653fa156-ee21-43f9-b9a7-8804c80f403a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBA_reviews.xls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# If it's an .xls file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Check the first few rows\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1567\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[1;32m-> 1567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_xlrd.py:45\u001b[0m, in \u001b[0;36mXlrdReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mReader using xlrd engine.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall xlrd >= 2.0.1 for xls Excel support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     47\u001b[0m     filepath_or_buffer,\n\u001b[0;32m     48\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m     49\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     50\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"BA_reviews.xls\", engine=\"xlrd\")  # If it's an .xls file\n",
    "print(df.head())  # Check the first few rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b875f-a315-4bb5-bc75-d18c5da0d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Ensure 'data' directory exists\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# Load dataset and check for encoding issues\n",
    "try:\n",
    "    df = pd.read_csv(\"data/BA_reviews.csv\", encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(\"data/BA_reviews.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Print column names to check\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Rename column if necessary\n",
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "if \"Review Text\" in df.columns:\n",
    "    df.rename(columns={\"Review Text\": \"Review\"}, inplace=True)\n",
    "\n",
    "# Check if 'Review' column exists\n",
    "if \"Review\" not in df.columns:\n",
    "    raise KeyError(\"The 'Review' column is missing. Please check the dataset.\")\n",
    "\n",
    "# Fill NaN values\n",
    "df[\"Review\"] = df[\"Review\"].fillna(\"\")\n",
    "\n",
    "# Sentiment Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return \"Positive\" if analysis.sentiment.polarity > 0 else \"Negative\" if analysis.sentiment.polarity < 0 else \"Neutral\"\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df[\"Sentiment\"] = df[\"Review\"].apply(get_sentiment)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"data/cleaned_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa555f52-e8ba-4faf-a9e7-5ebd8eb3b02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafdd56-e322-48aa-ac0c-16fa20019d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e2967-2dc7-4417-93f4-ff5f3b30b700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a343c69-e14f-4d9d-b90d-9435acf1cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e386b8-ca1c-458d-8c92-1598fad7de06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
